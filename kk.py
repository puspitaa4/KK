# -*- coding: utf-8 -*-
"""KK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmUSk2955413qBegsGjJSxUO4bk29Jjh

# Deteksi Dini Penyakit Stroke menggunakan Naive Bayes berdasarkan Dataset Kesehatan dengan normalisasi MinMax

## Data Understanding

### Inisialisasi data awal
"""

import pandas as pd

# Baca file Excel
df = pd.read_csv("/content/healthcare-dataset-stroke-data.csv", na_values=['N/A'])

# Lihat isi data
print(df.info())

print("Visualisasi Data:")
print(df.head())

"""### Visualisasi setelah di drop beberapa kolom"""

import pandas as pd

# Baca file Excel
df = pd.read_csv("/content/healthcare-dataset-stroke-data.csv")


# Drop kolom yang tidak diinginkan
df = df.drop(columns=['ever_married', 'work_type', 'Residence_type'])

# Lihat isi data
print(df.info())

print("Visualisasi Data:")
print(df)

"""## Data Preprocessing

### Penanganan Missing Value
"""

import numpy as np
import pandas as pd

# Misalnya df sudah dibuat, ubah 'N/A' menjadi NaN
df = df.replace('N/A', np.nan)

# Hitung jumlah missing value (NaN) per kolom
missing_values = df.isnull().sum()
print("Jumlah missing value (termasuk 'N/A') per kolom:")
print(missing_values)

"""### Standarisasi Data

Disini kita melakukan normalisasi data dengan menggunakan MinMaxScaler dari library sklearn sekaligus melakukan penanganan missing value
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Tangani missing value pada kolom 'bmi' dengan imputasi (mean, bisa juga median)
df['bmi'] = df['bmi'].fillna(df['bmi'].median())

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Normalisasi kolom 'bmi' (hasilnya dalam bentuk array, ubah jadi Series)
df['bmi'] = scaler.fit_transform(df[['bmi']])

# Tampilkan hasil
print(df)

"""### Encoding Label"""

from sklearn.preprocessing import LabelEncoder

# Inisialisasi LabelEncoder
le = LabelEncoder()

# Label Encoding untuk kolom 'gender'
df['gender'] = le.fit_transform(df['gender'])

# Label Encoding untuk kolom 'smoking_status'
df['smoking_status'] = le.fit_transform(df['smoking_status'])

# Cek hasilnya
print(df)

"""#Pisahkan Fitur & Target"""

# Misalkan df adalah DataFrame kamu dan 'stroke' adalah target
X = df.drop(columns=['stroke'])  # Fitur
y = df['stroke']                 # Target

"""## Splitting Data"""

from sklearn.model_selection import train_test_split


# 2. Split data: 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""## Modelling menggunakan Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

# 3. Inisialisasi dan latih model Naive Bayes
model = GaussianNB()
model.fit(X_train, y_train)

# 4. Prediksi dan evaluasi
y_pred = model.predict(X_test)

# Evaluasi akurasi dan laporan klasifikasi
print("Akurasi:", accuracy_score(y_test, y_pred))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred))

# Hitung confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Buat heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""### Simpan hasil modelling ke file pkl"""

import joblib

# Misalnya model sudah dilatih
# model = GaussianNB()
# model.fit(X_train, y_train)

# Simpan model ke file .pkl
joblib.dump(model, 'naive_bayes_stroke_model.pkl')

print("Model berhasil disimpan ke naive_bayes_stroke_model.pkl")

"""## Evaluasi Data

Model Naive Bayes diatas menghasilkan akurasi sebesar 93,15%, yang berarti dari seluruh data uji, sebanyak 93,15% prediksi model sesuai dengan label sebenarnya (stroke atau tidak stroke)
"""